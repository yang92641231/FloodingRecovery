{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import repeat\n",
    "from multiprocessing import cpu_count, freeze_support\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取并设置索引 \n",
    "wide_data_avrg_2018 = pd.read_csv(\n",
    "    r'E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\dbf\\2018xy\\summary_wide.csv',\n",
    "    index_col=False\n",
    ")\n",
    "wide_data_avrg_2018 = wide_data_avrg_2018.set_index('xy_id')\n",
    "\n",
    "wide_data_avrg_2017 = pd.read_csv(\n",
    "    r'E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\dbf\\2017xy\\summary_wide.csv',\n",
    "    index_col=False\n",
    ")\n",
    "wide_data_avrg_2017 = wide_data_avrg_2017.set_index('xy_id')\n",
    "\n",
    "# 合并数据集\n",
    "wide_data_avrg = pd.concat([wide_data_avrg_2017, wide_data_avrg_2018], axis=1)\n",
    "del wide_data_avrg_2017, wide_data_avrg_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整灯光值\n",
    "wide_data_avrg.iloc[:, 1:] = wide_data_avrg.iloc[:, 1:] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  删除行均值小于0.5的数据\n",
    "row_means = wide_data_avrg.mean(axis=1, skipna=True)\n",
    "cleaned_wide_data_avrg = wide_data_avrg[row_means >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对异常值进行清理\n",
    "def remove_outliers_mad(\n",
    "    df, \n",
    "    k=5, \n",
    "    min_valid=3, \n",
    "    max_offset=3,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    移除时间序列中的局部异常值（基于中位数 + MAD）\n",
    "    \n",
    "    参数:\n",
    "    - df: pd.DataFrame，每列为时间（应为datetime格式），每行为一个序列\n",
    "    - k: 异常值判定的阈值倍数，默认5\n",
    "    - min_valid: 最少需要几个邻近有效值才能判断异常\n",
    "    - max_offset: 最多从 t 向前向后扩展几天以寻找邻近值\n",
    "    - verbose: 如果为True，会打印一些检测信息（默认False）\n",
    "    \n",
    "    返回:\n",
    "    - 一个清洗后的新DataFrame（异常值设为NaN）\n",
    "    \"\"\"\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    for i, row_idx in enumerate(df.index):\n",
    "        series = df.loc[row_idx]\n",
    "        for t_idx in range(len(series)):\n",
    "            neighbors = []\n",
    "            offset = 1\n",
    "\n",
    "            while offset <= max_offset and len(neighbors) < min_valid:\n",
    "                for delta in [-offset, offset]:\n",
    "                    neighbor_idx = t_idx + delta\n",
    "                    if 0 <= neighbor_idx < len(series):\n",
    "                        val = series.iloc[neighbor_idx]\n",
    "                        if pd.notna(val):\n",
    "                            neighbors.append(val)\n",
    "                offset += 1\n",
    "\n",
    "            if len(neighbors) >= min_valid and pd.notna(series.iloc[t_idx]):\n",
    "                median = np.median(neighbors)\n",
    "                mad = np.median([abs(x - median) for x in neighbors])\n",
    "                if mad > 0 and abs(series.iloc[t_idx] - median) > k * mad:\n",
    "                    df_cleaned.iat[i, t_idx] = np.nan\n",
    "                    if verbose:\n",
    "                        print(f\"Row {row_idx}, Column {df.columns[t_idx]} marked as outlier: {series.iloc[t_idx]}\")\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "cleaned_wide_data_avrg = remove_outliers_mad(cleaned_wide_data_avrg, k=5, min_valid=3, max_offset=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_wide_data_avrg.to_csv(r'E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\0520test\\cleaneddata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_wide_data_avrg = pd.read_csv(r'E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\0520test\\cleaneddata.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  1. 备份数据\n",
    "if 'cleaned_wide_data_avrg' not in globals():\n",
    "    print(\"！错误：在当前环境未找到 cleaned_wide_data_avrg，请先读取\")\n",
    "    print(\"cleaned_wide_data_avrg = pd.read_csv(..., index_col=0)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df = cleaned_wide_data_avrg\n",
    "df.columns = pd.to_datetime(df.columns)\n",
    "\n",
    "df_smooth = df.rolling(7, axis=1, min_periods=1).mean()\n",
    "df = df_smooth\n",
    "\n",
    "# —— A. 先验超参数 —— \n",
    "prior = dict(\n",
    "    trend = dict(\n",
    "        maxK = 5,            # 最多 5 个断点\n",
    "        minSepDistTrend = 10, # 相邻断点≥10 天\n",
    "        maxOrder = 1         # 段内线性\n",
    "    ),\n",
    "    alpha1 = 3,\n",
    "    alpha2 = 3\n",
    ")\n",
    "\n",
    "\n",
    "dates = df.columns\n",
    "freq  = 1\n",
    "mask_num = 0.1\n",
    "season_num = 4\n",
    "formula_type = 'y ~ trend+season' # or 'y ~ trend + season'\n",
    "\n",
    "json_out_path = r\"E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\0520test\\beast_model_0521.json\"\n",
    "out_trend_path = r\"E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\0520test\\beast_trends_0521.csv\"\n",
    "out_cp_path    = r\"E:\\National University of Singapore\\Yang Yang - flooding\\Process Data\\h09v06_Florida\\0520test\\beast_change_points_0521.csv\"\n",
    "\n",
    "results = []\n",
    "cp_records = []\n",
    "\n",
    "# —— 2. BEAST 单序列函数 —— \n",
    "\n",
    "def extract_cp_info(res):\n",
    "    import numpy as np\n",
    "\n",
    "    try:\n",
    "        cp_raw = getattr(res.trend, \"cp\", None)\n",
    "        print(f\"[DEBUG] raw cp = {cp_raw} ({type(cp_raw)})\")\n",
    "\n",
    "        # 转换为 ndarray\n",
    "        cp_array = np.asarray(cp_raw).ravel()\n",
    "        print(f\"[DEBUG] cp_array (raveled) = {cp_array}\")\n",
    "\n",
    "        # 去除 NaN\n",
    "        cp_array_clean = cp_array[~np.isnan(cp_array)]\n",
    "        print(f\"[DEBUG] cp_array_clean = {cp_array_clean}\")\n",
    "\n",
    "        # 转换为 int index\n",
    "        cp_index = cp_array_clean.astype(int)\n",
    "        print(f\"[DEBUG] cp_index = {cp_index}\")\n",
    "\n",
    "        # 再提取概率\n",
    "        cp_prob_full = getattr(res.trend, \"cpOccPr\", None)\n",
    "        cp_prob_array = np.asarray(cp_prob_full).ravel()\n",
    "        print(f\"[DEBUG] cpOccPr full = {cp_prob_array} (len={len(cp_prob_array)})\")\n",
    "\n",
    "        if cp_index.size > 0 and cp_index.max() < len(cp_prob_array):\n",
    "            cp_prob = cp_prob_array[cp_index]\n",
    "        else:\n",
    "            cp_prob = np.array([], dtype=float)\n",
    "\n",
    "        print(f\"[OK] extracted cp_index = {cp_index}, cp_prob = {cp_prob}\")\n",
    "        return cp_index, cp_prob\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR in extract_cp_info]:\", e)\n",
    "        return np.array([], dtype=int), np.array([], dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "def run_beast_on_series(h3_id, values, dates, freq=1):\n",
    "    import Rbeast, numpy as np, pandas as pd\n",
    "    \n",
    "\n",
    "\n",
    "    ts = pd.Series(values.astype(float), index=dates)\n",
    "\n",
    "    res = Rbeast.beast(\n",
    "        ts,\n",
    "        freq=freq,\n",
    "        season=season_num,\n",
    "        what='all',\n",
    "        formula=formula_type,\n",
    "        prior=prior,\n",
    "        extra={\n",
    "            \"computeCredible\": True,\n",
    "            \"computeTrendChngpt\": True,\n",
    "            \"computeTrendSlope\": True,\n",
    "            \"computeSeasonOrder\": True,\n",
    "            \"computeTrendOrder\": True,\n",
    "            \"computeSeasonChngpt\": True,\n",
    "            \"quiet\": True,\n",
    "            \"printProgress\": False,\n",
    "            \"dumpInputData\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    def _to_list(comp):\n",
    "        import numpy as np, collections.abc as cabc\n",
    "        if isinstance(comp, np.ndarray):\n",
    "            return comp.tolist()\n",
    "        for attr in (\"est\", \"value\", \"mean\", \"Y\", \"Yhat\"):  # 加上 Yhat\n",
    "            if hasattr(comp, attr):\n",
    "                return np.asarray(getattr(comp, attr)).squeeze().tolist()\n",
    "        if isinstance(comp, (list, tuple, cabc.Iterable)):\n",
    "            return list(comp)\n",
    "        raise TypeError(f\"Unrecognized BEAST component: {type(comp)}\")\n",
    "\n",
    "    trend_vals  = _to_list(res.trend)\n",
    "    fitted_vals = _to_list(res.fitted) if hasattr(res, \"fitted\") else None\n",
    "\n",
    "    # 过滤显著突变点\n",
    "    # ---------- 过滤显著突变点 ----------\n",
    "    cp_index, cp_prob = extract_cp_info(res)        # 1-based\n",
    "    # ---- 获取 slope （兼容多版本） ----\n",
    "    slope_arr = getattr(res.trend, \"slope\", None) or getattr(res.trend, \"slp\", None)\n",
    "    if slope_arr is None:        # 兜底：用差分代替\n",
    "        slope_arr = np.diff(trend_vals, prepend=trend_vals[0])\n",
    "    slope = np.asarray(slope_arr).ravel()       # 0-based\n",
    "\n",
    "    if cp_index.size:\n",
    "        # ① 1-based → 0-based\n",
    "        cp_index0 = cp_index - 1\n",
    "\n",
    "        # ② 剔除越界 or 负索引（第一天没前值）\n",
    "        valid     = cp_index0 > 0\n",
    "        cp_index0 = cp_index0[valid]\n",
    "        cp_prob   = cp_prob[valid]\n",
    "\n",
    "        # ③ 用“右侧 win 天平均斜率”判定下降\n",
    "        win = 3\n",
    "        right_mean = [\n",
    "            slope[i : min(i + win, len(slope))].mean()\n",
    "            for i in cp_index0\n",
    "        ]\n",
    "        right_mean = np.asarray(right_mean)\n",
    "\n",
    "        # ④ 向下跳且 right_mean 非 nan\n",
    "        down_mask  = (~np.isnan(right_mean)) & (right_mean < 0)\n",
    "        cp_index0  = cp_index0[down_mask]\n",
    "        cp_prob    = cp_prob[down_mask]\n",
    "    else:\n",
    "        cp_index0 = np.array([], dtype=int)\n",
    "        cp_prob   = np.array([], dtype=float)\n",
    "\n",
    "    # ⑤ 置信度筛选\n",
    "    mask          = cp_prob >= mask_num           \n",
    "    cp_index_keep = cp_index0[mask].tolist()\n",
    "    cp_prob_keep  = cp_prob[mask].tolist()\n",
    "    cp_date_keep  = [dates[i] for i in cp_index_keep]\n",
    "\n",
    "    print(f\"[{h3_id}] cp_date_keep: {cp_date_keep}\")\n",
    "\n",
    "    def _py(v):\n",
    "        import numpy as np\n",
    "        if isinstance(v, np.generic):\n",
    "            return v.item()\n",
    "        if isinstance(v, np.ndarray):\n",
    "            return v.tolist()\n",
    "        return v\n",
    "    \n",
    "    return {k: _py(v) for k, v in {\n",
    "        \"id\": h3_id,\n",
    "        \"trend\": trend_vals,\n",
    "        \"fitted\": fitted_vals,\n",
    "        \"cp_index\": cp_index_keep,\n",
    "        \"cp_date\":  [d.strftime(\"%Y-%m-%d\") for d in cp_date_keep],\n",
    "        \"cp_prob\":  cp_prob_keep,\n",
    "        \"r2\": getattr(res, \"R2\", None),\n",
    "        \"rmse\": getattr(res, \"RMSE\", None),\n",
    "        \"mape\": getattr(res, \"MAPE\", None),\n",
    "        \"n_cp\": len(cp_index_keep),\n",
    "        \"season_order\": None,\n",
    "        \"freq\": freq,\n",
    "    }.items()}\n",
    "\n",
    "\n",
    "#3. 串行处理，容错，中途储存\n",
    "\n",
    "\n",
    "# 断点续联\n",
    "completed = set()\n",
    "if os.path.exists(out_trend_path):\n",
    "    completed = set(pd.read_csv(out_trend_path, index_col=0).index)\n",
    "\n",
    "for idx in tqdm(df.index, desc=\"Serial BEAST\"):\n",
    "    if idx in completed:\n",
    "        continue  \n",
    "    values = df.loc[idx].to_numpy()\n",
    "    try:\n",
    "        r = run_beast_on_series(idx, values, dates, freq)\n",
    "        results.append(r)\n",
    "        \n",
    "        trend_series = pd.Series(\n",
    "            data=r['trend'], \n",
    "            index=dates, \n",
    "            name=r['id']\n",
    "        )\n",
    "        trend_series.to_frame().T.to_csv(\n",
    "            out_trend_path,\n",
    "            mode='a',\n",
    "            header=not os.path.exists(out_trend_path)\n",
    "        )\n",
    "        # 写入突变点\n",
    "        # 每格直接写入断点 CSV\n",
    "        if r[\"n_cp\"] > 0:\n",
    "            pd.DataFrame(\n",
    "                [{\"H3_id\": r[\"id\"], \"cp_date\": d, \"cp_prob\": p}\n",
    "                for d, p in zip(r[\"cp_date\"], r[\"cp_prob\"])]\n",
    "            ).to_csv(\n",
    "                out_cp_path,\n",
    "                mode='a',\n",
    "                index=False,\n",
    "                header=not os.path.exists(out_cp_path)\n",
    "            )\n",
    "            \n",
    "        with open(json_out_path, \"a\", encoding=\"utf-8\") as jf:\n",
    "            jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")        \n",
    "    except Exception as e:\n",
    "        print(f\"!! H3 {idx} 出错，跳过：{e}\")\n",
    "\n",
    "# 4.把剩余的 cp_records 写入\n",
    "if cp_records:\n",
    "    pd.DataFrame(cp_records).to_csv(out_cp_path, mode='a', index=False,\n",
    "                                   header=not os.path.exists(out_cp_path))\n",
    "print(\"Done\")\n",
    "print(f\"  趋势：{out_trend_path}\")\n",
    "print(f\"  突变点：{out_cp_path}\")\n",
    "\n",
    "\n",
    "print(f\"模型完整参数输出至：{json_out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
